{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Writing your own Tuner to support a custom training loop\n",
    "\n",
    "**Authors:** Tom O'Malley, Haifeng Jin<br>\n",
    "**Date created:** 2019/10/28<br>\n",
    "**Last modified:** 2021/06/02<br>\n",
    "**Description:** Subclassing the Tuner class in KerasTuner for more customization like custom training loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "The `Tuner` class at `keras_tuner.engine.tuner.Tuner` can be subclassed to\n",
    "support advanced uses such as:\n",
    "\n",
    "* Custom training loops (GANs, reinforement learning, etc.)\n",
    "* Adding hyperparameters outside of the model builing function (preprocessing,\n",
    "data augmentation, test time augmentation, etc.)\n",
    "\n",
    "This tutorial will not cover subclassing to support non-Keras models. To\n",
    "accomplish this, you can subclass the `keras_tuner.engine.base_tuner.BaseTuner`\n",
    "class (See `keras_tuner.tuners.sklearn.Sklearn` for an example).\n",
    "\n",
    "## Understanding the search process\n",
    "\n",
    "`Tuner.search` can be passed any arguments. These arguments will be passed\n",
    "directly to `Tuner.run_trial`, along with a `Trial` object that contains\n",
    "information about the current trial, including hyperparameters and the status\n",
    "of the trial. Typically, `Tuner.run_trial` is the only method that users need\n",
    "to override when subclassing `Tuner`.\n",
    "\n",
    "## Overriding `run_trial`\n",
    "\n",
    "There are two ways to write `run_trial`. One is to leverage `Tuner`'s built-in\n",
    "callback hooks, which send the value of the `objective` to the `Oracle` and\n",
    "save the latest state of the Model. These hooks are:\n",
    "\n",
    "* `self.on_epoch_end`: Must be called. Reports results to the `Oracle` and\n",
    "saves the Model. The `logs` dictionary passed to this method must contain the\n",
    "`objective` name.\n",
    "* `self.on_epoch_begin`, `self.on_batch_begin`, `self.on_batch_end`: Optional.\n",
    "These methods do nothing in `Tuner`, but are useful to provide as hooks if you\n",
    "expect users of your subclass to create their own subclasses that override\n",
    "these parts of the training process.\n",
    "\n",
    "\n",
    "```python\n",
    "class MyTuner(kt.Tuner):\n",
    "\n",
    "    def run_trial(self, trial, ...):\n",
    "        model = self.hypermodel.build(trial.hyperparameters)\n",
    "        for epoch in range(10):\n",
    "              epoch_loss = ...\n",
    "              self.on_epoch_end(trial, model, epoch, logs={'loss': epoch_loss})\n",
    "```\n",
    "\n",
    "Alternatively, you can instead directly call the methods used to report results\n",
    "to the `Oracle` and save the Model. This can allow more flexibility for use\n",
    "cases where there is no natural concept of epoch or where you do not want to\n",
    "report results to the `Oracle` after each epoch. These methods are:\n",
    "\n",
    "* `self.oracle.update_trial`: Reports current results to the `Oracle`. The\n",
    "`metrics` dictionary passed to this method must contain the `objective` name.\n",
    "* `self.save_model`: Saves the trained model.\n",
    "\n",
    "```python\n",
    "class MyTuner(kt.Tuner):\n",
    "\n",
    "    def run_trial(self, trial, ...):\n",
    "        model = self.hypermodel.build(trial.hyperparameters)\n",
    "        score = ...\n",
    "        self.oracle.update_trial(trial.trial_id, {'score': score})\n",
    "        self.save_model(trial.trial_id, model)\n",
    "```\n",
    "\n",
    "### Adding HyperParameters during preprocessing, evaluation, etc.\n",
    "\n",
    "New `HyperParameter`s can be defined anywhere in `run_trial`, in the same way\n",
    "that `HyperParameter`s are defined in a `HyperModel`. These hyperparameters\n",
    "take on their default value the first time they are encountered, and thereafter\n",
    "are tuned by the `Oracle`.\n",
    "\n",
    "```python\n",
    "class MyTuner(kt.Tuner):\n",
    "\n",
    "    def run_trial(self, trial, ...):\n",
    "        hp = trial.hyperparameters\n",
    "        model = self.hypermodel.build(hp)\n",
    "\n",
    "        batch_size = hp.Int('batch_size', 32, 128, step=32)\n",
    "        random_flip = hp.Boolean('random_flip')\n",
    "        ...\n",
    "```\n",
    "\n",
    "### End-to-end Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    \"\"\"Builds a convolutional model.\"\"\"\n",
    "    inputs = tf.keras.Input(shape=(28, 28, 1))\n",
    "    x = inputs\n",
    "    for i in range(hp.Int(\"conv_layers\", 1, 3, default=3)):\n",
    "        x = tf.keras.layers.Conv2D(\n",
    "            filters=hp.Int(\"filters_\" + str(i), 4, 32, step=4, default=8),\n",
    "            kernel_size=hp.Int(\"kernel_size_\" + str(i), 3, 5),\n",
    "            activation=\"relu\",\n",
    "            padding=\"same\",\n",
    "        )(x)\n",
    "\n",
    "        if hp.Choice(\"pooling\" + str(i), [\"max\", \"avg\"]) == \"max\":\n",
    "            x = tf.keras.layers.MaxPooling2D()(x)\n",
    "        else:\n",
    "            x = tf.keras.layers.AveragePooling2D()(x)\n",
    "\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    if hp.Choice(\"global_pooling\", [\"max\", \"avg\"]) == \"max\":\n",
    "        x = tf.keras.layers.GlobalMaxPooling2D()(x)\n",
    "    else:\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = tf.keras.layers.Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    optimizer = hp.Choice(\"optimizer\", [\"adam\", \"sgd\"])\n",
    "    model.compile(\n",
    "        optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "class MyTuner(kt.Tuner):\n",
    "    def run_trial(self, trial, train_ds):\n",
    "        hp = trial.hyperparameters\n",
    "\n",
    "        # Hyperparameters can be added anywhere inside `run_trial`.\n",
    "        # When the first trial is run, they will take on their default values.\n",
    "        # Afterwards, they will be tuned by the `Oracle`.\n",
    "        train_ds = train_ds.batch(hp.Int(\"batch_size\", 32, 128, step=32, default=64))\n",
    "\n",
    "        model = self.hypermodel.build(trial.hyperparameters)\n",
    "        lr = hp.Float(\"learning_rate\", 1e-4, 1e-2, sampling=\"log\", default=1e-3)\n",
    "        optimizer = tf.keras.optimizers.Adam(lr)\n",
    "        epoch_loss_metric = tf.keras.metrics.Mean()\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "        # @tf.function\n",
    "        def run_train_step(data):\n",
    "            images = tf.dtypes.cast(data[0], \"float32\") / 255.0\n",
    "            labels = data[1]\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(images)\n",
    "                loss = loss_fn(labels, logits)\n",
    "                # Add any regularization losses.\n",
    "                if model.losses:\n",
    "                    loss += tf.math.add_n(model.losses)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            epoch_loss_metric.update_state(loss)\n",
    "            return loss\n",
    "\n",
    "        # `self.on_epoch_end` reports results to the `Oracle` and saves the\n",
    "        # current state of the Model. The other hooks called here only log values\n",
    "        # for display but can also be overridden. For use cases where there is no\n",
    "        # natural concept of epoch, you do not have to call any of these hooks. In\n",
    "        # this case you should instead call `self.oracle.update_trial` and\n",
    "        # `self.oracle.save_model` manually.\n",
    "        for epoch in range(2):\n",
    "            print(\"Epoch: {}\".format(epoch))\n",
    "\n",
    "            self.on_epoch_begin(trial, model, epoch, logs={})\n",
    "            for batch, data in enumerate(train_ds):\n",
    "                self.on_batch_begin(trial, model, batch, logs={})\n",
    "                batch_loss = float(run_train_step(data))\n",
    "                self.on_batch_end(trial, model, batch, logs={\"loss\": batch_loss})\n",
    "\n",
    "                if batch % 100 == 0:\n",
    "                    loss = epoch_loss_metric.result().numpy()\n",
    "                    print(\"Batch: {}, Average Loss: {}\".format(batch, loss))\n",
    "\n",
    "            epoch_loss = epoch_loss_metric.result().numpy()\n",
    "            self.on_epoch_end(trial, model, epoch, logs={\"loss\": epoch_loss})\n",
    "            epoch_loss_metric.reset_states()\n",
    "\n",
    "\n",
    "tuner = MyTuner(\n",
    "    oracle=kt.oracles.BayesianOptimization(\n",
    "        objective=kt.Objective(\"loss\", \"min\"), max_trials=2\n",
    "    ),\n",
    "    hypermodel=build_model,\n",
    "    directory=\"results\",\n",
    "    project_name=\"mnist_custom_training\",\n",
    ")\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Reshape the images to have the channel dimension.\n",
    "x_train = x_train.reshape(x_train.shape + (1,))[:1000]\n",
    "y_train = y_train.astype(np.int64)[:1000]\n",
    "\n",
    "mnist_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "\n",
    "tuner.search(train_ds=mnist_train)\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters()[0]\n",
    "print(best_hps.values)\n",
    "\n",
    "best_model = tuner.get_best_models()[0]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "custom_tuner",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}