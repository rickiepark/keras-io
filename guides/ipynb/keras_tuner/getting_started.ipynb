{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Getting started with KerasTuner\n",
    "\n",
    "**Authors:** Luca Invernizzi, James Long, Francois Chollet, Tom O'Malley, Haifeng Jin<br>\n",
    "**Date created:** 2019/05/31<br>\n",
    "**Last modified:** 2021/06/07<br>\n",
    "**Description:** The basics of using KerasTuner to tune model hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!pip install keras-tuner -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Here's how to perform hyperparameter tuning for a single-layer dense neural\n",
    "network using random search.\n",
    "First, we need to prepare the dataset -- let's use MNIST dataset as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "(x, y), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x[:-10000]\n",
    "x_val = x[-10000:]\n",
    "y_train = y[:-10000]\n",
    "y_val = y[-10000:]\n",
    "\n",
    "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255.0\n",
    "x_val = np.expand_dims(x_val, -1).astype(\"float32\") / 255.0\n",
    "x_test = np.expand_dims(x_test, -1).astype(\"float32\") / 255.0\n",
    "\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Prepare a model-building function\n",
    "\n",
    "Then, we define a model-building function. It takes an argument `hp` from\n",
    "which you can sample hyperparameters, such as\n",
    "`hp.Int('units', min_value=32, max_value=512, step=32)`\n",
    "(an integer from a certain range).\n",
    "\n",
    "This function returns a compiled model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from keras_tuner import RandomSearch\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(\n",
    "        layers.Dense(\n",
    "            units=hp.Int(\"units\", min_value=32, max_value=512, step=32),\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "    )\n",
    "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "        ),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Start the search\n",
    "\n",
    "Next, let's instantiate a tuner. You should specify the model-building function, the\n",
    "name of the objective to optimize (whether to minimize or maximize is\n",
    "automatically inferred for built-in metrics), the total number of trials\n",
    "(`max_trials`) to test, and the number of models that should be built and fit\n",
    "for each trial (`executions_per_trial`).\n",
    "\n",
    "We use the `overwrite` argument to control whether to overwrite the previous\n",
    "results in the same directory or resume the previous search instead.  Here we\n",
    "set `overwrite=True` to start a new search and ignore any previous results.\n",
    "\n",
    "Available tuners are `RandomSearch`, `BayesianOptimization` and `Hyperband`.\n",
    "\n",
    "**Note:** the purpose of having multiple executions per trial is to reduce\n",
    "results variance and therefore be able to more accurately assess the\n",
    "performance of a model. If you want to get results faster, you could set\n",
    "`executions_per_trial=1` (single round of training for each model\n",
    "configuration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=3,\n",
    "    executions_per_trial=2,\n",
    "    overwrite=True,\n",
    "    directory=\"my_dir\",\n",
    "    project_name=\"helloworld\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "You can print a summary of the search space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Then, start the search for the best hyperparameter configuration.\n",
    "The call to `search` has the same signature as `model.fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "tuner.search(x_train, y_train, epochs=2, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Here's what happens in `search`: models are built iteratively by calling the\n",
    "model-building function, which populates the hyperparameter space (search\n",
    "space) tracked by the `hp` object. The tuner progressively explores the space,\n",
    "recording metrics for each configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Query the results\n",
    "\n",
    "When search is over, you can retrieve the best model(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "models = tuner.get_best_models(num_models=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Or print a summary of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "You will also find detailed logs, checkpoints, etc, in the folder `my_dir/helloworld`, i.e. `directory/project_name`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## The search space may contain conditional hyperparameters\n",
    "\n",
    "Below, we have a `for` loop creating a tunable number of layers,\n",
    "which themselves involve a tunable `units` parameter.\n",
    "\n",
    "This can be pushed to any level of parameter interdependency, including recursion.\n",
    "\n",
    "Note that all parameter names should be unique (here, in the loop over `i`,\n",
    "we name the inner parameters `'units_' + str(i)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten())\n",
    "    for i in range(hp.Int(\"num_layers\", 2, 20)):\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                units=hp.Int(\"units_\" + str(i), min_value=32, max_value=512, step=32),\n",
    "                activation=\"relu\",\n",
    "            )\n",
    "        )\n",
    "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(hp.Choice(\"learning_rate\", [1e-2, 1e-3, 1e-4])),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## You can use a HyperModel subclass instead of a model-building function\n",
    "\n",
    "This makes it easy to share and reuse hypermodels.\n",
    "\n",
    "A `HyperModel` subclass only needs to implement a `build(self, hp)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from keras_tuner import HyperModel\n",
    "\n",
    "\n",
    "class MyHyperModel(HyperModel):\n",
    "    def __init__(self, classes):\n",
    "        self.classes = classes\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                units=hp.Int(\"units\", min_value=32, max_value=512, step=32),\n",
    "                activation=\"relu\",\n",
    "            )\n",
    "        )\n",
    "        model.add(layers.Dense(self.classes, activation=\"softmax\"))\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(\n",
    "                hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "            ),\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "        return model\n",
    "\n",
    "\n",
    "hypermodel = MyHyperModel(classes=10)\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=3,\n",
    "    overwrite=True,\n",
    "    directory=\"my_dir\",\n",
    "    project_name=\"helloworld\",\n",
    ")\n",
    "\n",
    "tuner.search(x_train, y_train, epochs=2, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## KerasTuner includes pre-made tunable applications: HyperResNet and HyperXception\n",
    "\n",
    "These are ready-to-use hypermodels for computer vision.\n",
    "\n",
    "They come pre-compiled with `loss=\"categorical_crossentropy\"` and `metrics=[\"accuracy\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from keras_tuner.applications import HyperResNet\n",
    "\n",
    "hypermodel = HyperResNet(input_shape=(28, 28, 1), classes=10)\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=3,\n",
    "    overwrite=True,\n",
    "    directory=\"my_dir\",\n",
    "    project_name=\"helloworld\",\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    x_train[:100], y_train[:100], epochs=1, validation_data=(x_val[:100], y_val[:100])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## You can easily restrict the search space to just a few parameters\n",
    "\n",
    "If you have an existing hypermodel, and you want to search over only a few parameters\n",
    "(such as the learning rate), you can do so by passing a `hyperparameters` argument\n",
    "to the tuner constructor, as well as `tune_new_entries=False` to specify that parameters\n",
    "that you didn't list in `hyperparameters` should not be tuned. For these parameters, the default\n",
    "value gets used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from keras_tuner import HyperParameters\n",
    "from keras_tuner.applications import HyperXception\n",
    "\n",
    "hypermodel = HyperXception(input_shape=(28, 28, 1), classes=10)\n",
    "\n",
    "hp = HyperParameters()\n",
    "\n",
    "# This will override the `learning_rate` parameter with your\n",
    "# own selection of choices\n",
    "hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    hyperparameters=hp,\n",
    "    # `tune_new_entries=False` prevents unlisted parameters from being tuned\n",
    "    tune_new_entries=False,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=3,\n",
    "    overwrite=True,\n",
    "    directory=\"my_dir\",\n",
    "    project_name=\"helloworld\",\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    x_train[:100], y_train[:100], epochs=1, validation_data=(x_val[:100], y_val[:100])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## About parameter default values\n",
    "\n",
    "Whenever you register a hyperparameter inside a model-building function or the `build` method of a hypermodel,\n",
    "you can specify a default value:\n",
    "\n",
    "```python\n",
    "hp.Int(\"units\", min_value=32, max_value=512, step=32, default=128)\n",
    "```\n",
    "\n",
    "If you don't, hyperparameters always have a default default (for `Int`, it is equal to `min_value`).\n",
    "\n",
    "## Fixing values in a hypermodel\n",
    "\n",
    "What if you want to do the reverse -- tune all available parameters in a hypermodel, **except** one (the learning rate)?\n",
    "\n",
    "Pass a `hyperparameters` argument with a `Fixed` entry (or any number of `Fixed` entries), and specify `tune_new_entries=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "hypermodel = HyperXception(input_shape=(28, 28, 1), classes=10)\n",
    "\n",
    "hp = HyperParameters()\n",
    "hp.Fixed(\"learning_rate\", value=1e-4)\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    hyperparameters=hp,\n",
    "    tune_new_entries=True,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=3,\n",
    "    overwrite=True,\n",
    "    directory=\"my_dir\",\n",
    "    project_name=\"helloworld\",\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    x_train[:100], y_train[:100], epochs=1, validation_data=(x_val[:100], y_val[:100])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Overriding compilation arguments\n",
    "\n",
    "If you have a hypermodel for which you want to change the existing optimizer,\n",
    "loss, or metrics, you can do so by passing these arguments\n",
    "to the tuner constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "hypermodel = HyperXception(input_shape=(28, 28, 1), classes=10)\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss=\"mse\",\n",
    "    metrics=[\n",
    "        keras.metrics.Precision(name=\"precision\"),\n",
    "        keras.metrics.Recall(name=\"recall\"),\n",
    "    ],\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=3,\n",
    "    overwrite=True,\n",
    "    directory=\"my_dir\",\n",
    "    project_name=\"helloworld\",\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    x_train[:100], y_train[:100], epochs=1, validation_data=(x_val[:100], y_val[:100])\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "getting_started",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}